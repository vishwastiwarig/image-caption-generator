# üñºÔ∏è Image Caption Generator

‚Ä¢ **AI-powered image caption generator** that automatically describes images using CNN-LSTM deep learning model
‚Ä¢ **Combines computer vision and natural language processing** to generate human-like descriptions of images

## üåü Features

‚Ä¢ **Automatic Image Captioning** - Generate descriptive captions for any image
‚Ä¢ **Deep Learning Architecture** - CNN (Xception) + LSTM model
‚Ä¢ **Kaggle Ready** - Optimized for Kaggle notebooks with GPU support
‚Ä¢ **Interactive Testing** - Easy-to-use functions for testing your own images
‚Ä¢ **Visual Results** - Display images with generated captions
‚Ä¢ **Model Evaluation** - Built-in performance metrics and testing

## üèóÔ∏è Model Architecture

```
‚Ä¢ Image Input (299x299x3)
        ‚Üì
‚Ä¢ Xception CNN (Pre-trained)
        ‚Üì
‚Ä¢ Feature Vector (2048)
        ‚Üì
‚Ä¢ Dense Layer (256) ‚Üí Dropout
        ‚Üì
‚Ä¢ Text Input (max_length) ‚Üí Embedding (256) ‚Üí LSTM (256)
        ‚Üì
‚Ä¢ Combine Features (Add)
        ‚Üì
‚Ä¢ Dense (256) ‚Üí Output (vocab_size)
```

![Model Architecture](https://github.com/user-attachments/assets/78de3a55-dd75-48fd-ab13-00d6ca32ed80)

## üìä Dataset

‚Ä¢ **Dataset**: Flickr8k (8,091 images with 40,455 captions)
‚Ä¢ **Images**: Various scenes, people, animals, objects  
‚Ä¢ **Captions**: 5 human-written descriptions per image
‚Ä¢ **Vocabulary**: ~7,500 unique words after cleaning
‚Ä¢ **Max Caption Length**: 20-25 words



## üîß Technical Specifications

### Model Components
‚Ä¢ **CNN Backbone**: Xception (pre-trained on ImageNet)
‚Ä¢ **RNN**: LSTM with 256 hidden units
‚Ä¢ **Embedding**: 256-dimensional word embeddings
‚Ä¢ **Optimizer**: Adam (learning_rate=0.0005)
‚Ä¢ **Loss Function**: Categorical Crossentropy
‚Ä¢ **Batch Size**: 32-64
‚Ä¢ **Epochs**: 5-8

### Data Processing
‚Ä¢ **Image Preprocessing**: Resize to 299√ó299, normalize to [-1,1]
‚Ä¢ **Text Preprocessing**: Lowercase, remove punctuation, filter short words
‚Ä¢ **Sequence Padding**: Post-padding for GPU compatibility
‚Ä¢ **Vocabulary**: Top ~7,500 most frequent words

## üìà Performance Metrics

### Model Performance
‚Ä¢ **Training Accuracy**: 70-80%
‚Ä¢ **Validation Accuracy**: ~60-70%
‚Ä¢ **Average Caption Length**: 6-8 words
‚Ä¢ **Word Overlap with Ground Truth**: ~40-50%

### Load and Use Model
```python
‚Ä¢ # Load trained model
‚Ä¢ model = load_model('/kaggle/working/best_model.h5')
‚Ä¢ tokenizer = pickle.load(open('/kaggle/working/tokenizer.pkl', 'rb'))

‚Ä¢ # Generate caption
‚Ä¢ caption = generate_caption(model, tokenizer, image_features, max_length)
‚Ä¢ print(f"Generated: {caption}")
```

### Batch Processing
```python
‚Ä¢ # Process multiple images
‚Ä¢ image_ids = ["1000268201_693b08cb0e", "1001773457_577c3a7d70"]
‚Ä¢ for img_id in image_ids:
‚Ä¢     caption = test_any_dataset_image(img_id)
‚Ä¢     print(f"{img_id}: {caption}")
```

## üéØ Performance Tips

‚Ä¢ **Use GPU**: Enable T4 x2 in Kaggle for 10x faster training
‚Ä¢ **Larger Dataset**: Use all 8,091 images for better results  
‚Ä¢ **More Epochs**: Train for 8-10 epochs for optimal performance
‚Ä¢ **Temperature Sampling**: Use temperature=0.7 for diverse captions



## üìÑ License

‚Ä¢ This project is licensed under the **MIT License**

## üôè Acknowledgments

‚Ä¢ **Flickr8k Dataset**: University of Illinois
‚Ä¢ **Xception Model**: Google Research  
‚Ä¢ **TensorFlow/Keras**: Google
‚Ä¢ **Kaggle**: For free GPU compute

